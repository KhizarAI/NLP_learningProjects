{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKBV5_JFh6EV",
        "outputId": "dd7abff4-7353-4712-d0fa-2a450997316f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Preprocess Data**"
      ],
      "metadata": {
        "id": "FfsxXltcBgtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9etc8iAn-Cif"
      },
      "outputs": [],
      "source": [
        "def load_data(text):\n",
        "  with open(text, \"r\") as f:\n",
        "    data = f.read()\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/twitter.txt\"\n",
        "data = load_data(data_path)\n",
        "\n",
        "print(\"Length of the text corpus : \",len(data))\n",
        "print(\"Staring of the data : \", data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0TfVzJkBmcS",
        "outputId": "942b86f1-3531-4394-e39b-c5287e702292"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the text corpus :  3335477\n",
            "Staring of the data :  How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_sentences(data):\n",
        "  sentences = data.split('\\n')\n",
        "  sentences = [s.strip() for s in sentences]\n",
        "  sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "_NNJ2e8UDNmt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "  tokenize = []\n",
        "  for sentence in sentences:\n",
        "      sentence = sentence.lower()\n",
        "      tokenize_sentence = nltk.word_tokenize(sentence)\n",
        "      tokenize.append(tokenize_sentence)\n",
        "\n",
        "  return tokenize"
      ],
      "metadata": {
        "id": "B5UgeraxGwNX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data):\n",
        "  sentences = split_to_sentences(data)\n",
        "  tokenized_sentences = tokenize_sentences(sentences)\n",
        "  return tokenized_sentences"
      ],
      "metadata": {
        "id": "ld_swo7hHr7f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(data):\n",
        "  tokenized_data = tokenize_data(data)\n",
        "  random.shuffle(tokenized_data)\n",
        "  split_ratio = 0.8\n",
        "  split_index = int(len(tokenized_data) * split_ratio)\n",
        "  train_data = tokenized_data[:split_index]\n",
        "  test_data = tokenized_data[split_index:]\n",
        "\n",
        "  return tokenized_data, train_data, test_data"
      ],
      "metadata": {
        "id": "Oq2ItmNaKZ3M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_data, train_data, test_data = train_test_split(data)\n",
        "\n",
        "print(\"Length of tokenized data : \", len(tokenize_data))\n",
        "print(\"Length of train data     : \", len(train_data))\n",
        "print(\"Length of split data     : \", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS_fNFHlM1LC",
        "outputId": "7b617987-0874-4fd9-9df5-de5bab85849e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of tokenized data :  47961\n",
            "Length of train data     :  38368\n",
            "Length of split data     :  9593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tokenized_sentences):\n",
        "  counts = {}\n",
        "  for sentence in tokenized_sentences:\n",
        "    for token in sentence:\n",
        "      if token not in counts:\n",
        "        counts[token] = 1\n",
        "      else:\n",
        "        counts[token] += 1\n",
        "\n",
        "  return counts"
      ],
      "metadata": {
        "id": "-MxFmYs8NMue"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words_with_n_frequency(tokenized_sentences, threshold):\n",
        "  closed_vocab = []\n",
        "  word_counts = count_words(tokenized_sentences)\n",
        "  for word, count in word_counts.items():\n",
        "    if count >= threshold:\n",
        "      closed_vocab.append(word)\n",
        "\n",
        "  return closed_vocab"
      ],
      "metadata": {
        "id": "tscKdF-sPPvj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_with_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "  vocabulary = vocabulary\n",
        "  replaced_tokenized_sentences = []\n",
        "  for sentence in tokenized_sentences:\n",
        "    replaced_sentence = []\n",
        "    for token in sentence:\n",
        "      if token in vocabulary:\n",
        "        replaced_sentence.append(token)\n",
        "      else:\n",
        "        replaced_sentence.append(unknown_token)\n",
        "\n",
        "    replaced_tokenized_sentences.append(replaced_sentence)\n",
        "\n",
        "  return replaced_tokenized_sentences"
      ],
      "metadata": {
        "id": "l02Jav8BQEpM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\"):\n",
        "  vocabulary = words_with_n_frequency(train_data, count_threshold)\n",
        "  train_data_replaced = replace_oov_with_unk(train_data, vocabulary, unknown_token)\n",
        "  test_data_replaced = replace_oov_with_unk(test_data, vocabulary, unknown_token)\n",
        "\n",
        "  return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "metadata": {
        "id": "CIjVnx--i1p-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data,\n",
        "                                                                        test_data,\n",
        "                                                                        count_threshold=2)"
      ],
      "metadata": {
        "id": "unrC7d_nf5Sy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Develop n-gram based Language Models**"
      ],
      "metadata": {
        "id": "z_r02EnPil5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_gram(data, n, start_token='<s>', end_token = '<e>'):\n",
        "  n_grams = {}\n",
        "  for sentence in data:\n",
        "    sentence = [start_token] * n + sentence + [end_token]\n",
        "    sentence = tuple(sentence)\n",
        "    for i in range(len(sentence) - n + 1):\n",
        "      gram = sentence[i:i+n]\n",
        "      if gram in n_grams:\n",
        "        n_grams[gram] += 1\n",
        "      else:\n",
        "        n_grams[gram] = 1\n",
        "\n",
        "  return n_grams"
      ],
      "metadata": {
        "id": "NQ1mdve_iqSx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability_of_single_word(word, previous_n_gram,\n",
        "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "  previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "  denominator = previous_n_gram_count+k*vocabulary_size\n",
        "  n_plus1_gram = previous_n_gram + (word,)\n",
        "  n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "  numerator = n_plus1_gram_count+k\n",
        "  probability = numerator/denominator\n",
        "\n",
        "  return probability"
      ],
      "metadata": {
        "id": "dDie-I9YooGK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability_of_all_words(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "  vocabulary = vocabulary + ['<e>', '<unk>']\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  probabilities = {}\n",
        "  for word in vocabulary:\n",
        "    probability = estimate_probability_of_single_word(word, previous_n_gram,\n",
        "                                        n_gram_counts, n_plus1_gram_counts,\n",
        "                                        vocabulary_size, k=k)\n",
        "\n",
        "    probabilities[word] = probability\n",
        "\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "8UC6oOLTrVPz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "      n_gram = n_plus1_gram[0:-1]\n",
        "      n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "\n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "      n_gram = n_plus1_gram[0:-1]\n",
        "      word = n_plus1_gram[-1]\n",
        "      if word not in vocabulary:\n",
        "          continue\n",
        "      i = row_index[n_gram]\n",
        "      j = col_index[word]\n",
        "      count_matrix[i, j] = count\n",
        "\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ],
      "metadata": {
        "id": "_lHVZI2N1ZN3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "metadata": {
        "id": "d3MInIUSs6Pa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplexity**"
      ],
      "metadata": {
        "id": "erTygG6m1E9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token = '<e>', k=1.0):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    sentence = [start_token] * n + sentence + [end_token]\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)\n",
        "    product_pi = 1.0\n",
        "    for t in range(n, N):\n",
        "      n_gram = sentence[t-n:t]\n",
        "      word = sentence[t]\n",
        "      probability = estimate_probability_of_single_word(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
        "      product_pi *= 1/probability\n",
        "\n",
        "    perplexity = (product_pi)**(1/N)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "j0IrqsFf1mUq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto-complete System**"
      ],
      "metadata": {
        "id": "RjYw1yFI1JFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    previous_tokens = ['<s>'] * n + previous_tokens\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    probabilities = estimate_probability_of_all_words(previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "    for word, prob in probabilities.items():\n",
        "      if start_with is not None:\n",
        "        if not word.startswith(start_with):\n",
        "            continue\n",
        "\n",
        "      if prob >= max_prob:\n",
        "        suggestion = word\n",
        "        max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob"
      ],
      "metadata": {
        "id": "kuaH3aUX1pzj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "      n_gram_counts = n_gram_counts_list[i]\n",
        "      n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "      suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "      suggestions.append(suggestion)\n",
        "    return suggestions"
      ],
      "metadata": {
        "id": "Nkh98_Vt1sy5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 4):\n",
        "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
        "    n_model_counts = count_n_gram(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV_ZIHMpfYZ0",
        "outputId": "15430321-1c34-48a1-9734-4559c623250a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing n-gram counts with n = 1 ...\n",
            "Computing n-gram counts with n = 2 ...\n",
            "Computing n-gram counts with n = 3 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"Who\", \"are\"]\n",
        "suggest = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "print(suggest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiYCRK2wfo-2",
        "outputId": "3e7f0914-80fe-4dce-df2c-b9988726bd3b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are ['Who', 'are'], the suggestions are:\n",
            "[('you', 0.022645393721049924), ('<unk>', 6.75995403231258e-05)]\n"
          ]
        }
      ]
    }
  ]
}